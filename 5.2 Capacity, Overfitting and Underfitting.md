# 5.2 용량, 과적합 및 과소적합
## 머신러닝의 중심 과제
- 모델이 교육받은 입력뿐만 아니라 이전에는 볼 수 없었던 새로운 입력에 대해서도 잘 수행해야 한다.
- 즉, 일반화의 오류가 낮기를 원한다는 소리
- 일반화의 오류: 새 입력에 대한 오류의 예상 값
- 우리는 일반적으로 training set과 별도로 수집된 test set에서 성능을 측정하여 기계학습의 일반화 오류를 측정한다.

## 데이터 생성 과정
| i.i.d(independent identically distributed): 각각의 랜덤 변수들이 독립적이고 동일한 확률분포를 가지는 분포를 뜻함. ex)동전, 주사위 던지기
1. i.i.d 가정으로 알려진 일련의 가정을 만든다.
- training error와 test error 사이의 관계를 수학적으로 연구 할 수 있게 해주기 때문

2. 동일한 분포를 사용하여 모든 훈련 예제와 모든 테스트 예제를 생성한다.

## 알고리즘의 성능을 결정하는 요소
1. Training error를 작게 만드는 능력
2. Training error와 Test error의 차이를 작게 만드는 능력

- 이 두 요인은 머신러닝의 중심 과제인 언더피팅과 오버피팅에 해당한다.
- 우리는 모델의 용량을 변경하여 모델의 오버피팅과 언더피팅 가능성을 제어할 수 있다.

## Under-fitting
| 모델이 Traning set에서 충분히 낮은 오차 값을 얻을 수 없을 때 발생

"동그란 것은 공이야"라고 로봇에게 가르친다.
![](https://t1.daumcdn.net/cfile/tistory/99E311435B76CC931C)

학습 후, testing 단계에 돌입한다.
![](https://t1.daumcdn.net/cfile/tistory/9953B0385B76CC9411)
![](https://t1.daumcdn.net/cfile/tistory/99FBF5335B76CC9503)
![](https://t1.daumcdn.net/cfile/tistory/99DBD0435B76CC951D)
![](https://t1.daumcdn.net/cfile/tistory/99DD5C4B5B76CC9608)
![](https://t1.daumcdn.net/cfile/tistory/9945E5485B76CC962B)

공 = 동그란 것이라고만 학습되어 error가 발생한다. 이를 too Bias하게 학습된 모델이라고 한다.

## Over-fitting
| 훈련 오류와 시험 오류 사이의 간격이 너무 클 때 발생

로봇에게 동그랗고, 먹을 수 없으며 가지고 놀수 있는 것이 공이라고 학습시킨다.
![](https://t1.daumcdn.net/cfile/tistory/995481455B76CC972F)

여기에 공은 실밥이 있고 지름이 7cm이상이라는 지엽적인 특성까지 학습시킨다.
![](https://t1.daumcdn.net/cfile/tistory/99AD89485B76CC9724)

지나치게 학습되어 봤던 데이터는 잘 맞추는데 새로운 데이터는 예측하지 못 하는 결과를 초래했다.
이를 high variance한 모델이라고 한다.

## Under and Over-fitting examples
![](https://brunch.co.kr/@gimmesilver/44)
-왼쪽은 Under-fitting의 예시, 오른쪽은 Over-fitting의 예시이다.
- Under-fitting은 학습된 것이 별로 없어 error가 나는 것을 볼 수 있다.
- Over-fitting은 지나치게 학습이 되어 training set의 모든 것을 잘 맞추는 것을 볼 수 있다.
